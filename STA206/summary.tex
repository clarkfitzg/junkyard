% A simple template for LaTeX documents

\documentclass[12pt]{article}

% Begin paragraphs with new line
\usepackage{parskip}

% Change margin size
\usepackage[margin=1in]{geometry}

% Supports proof environment
\usepackage{amsthm}

% Allows writing \implies and align*
\usepackage{amsmath}

% Allows \mathbb{R}
\usepackage{amsfonts}

% Allows bold greek symbols \bm{\sigma} 
\usepackage{bm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% User defined commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\E}{\mathrm{E}}

\newcommand{\Var}{\mathrm{Var}}

\newcommand{\Cov}{\mathrm{Cov}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\section{Definitions}

\subsubsection{Model Assumptions}

The simple linear regression model with Normal errors assumes
\begin{itemize}
    \item $y = X \beta + \epsilon$ The response is a linear function of the predictors.
    \item $\E(\epsilon) = 0$ The expectation of the error term is 0.
    \item $\Cov(\epsilon) = \sigma^2 I_n$ The error terms have constant
variance $\sigma^2$ and are uncorrelated. This is equivalent to
$\Cov(y) = \sigma^2 I_n$.
\end{itemize}

The normal error model strengthens the second two conditions:
\begin{itemize}
    \item $\epsilon \sim $ Normal$(0, \sigma^2 I_n)$ The error terms are
normally distributed.
\end{itemize}

\subsubsection{Least Squares}

Least squares estimates $\beta$ by minimizing SSE, the error sum of
squares.
\[
    Q(\hat{\beta}) = e'e
\]

\subsubsection{Design matrix}

$y$ can be decomposed into orthogonal vectors
\[
    y = \hat{y} + e.
\]

Estimated $\hat{\beta}$.
\[
    \hat{\beta} = (X'X)^{-1}X'y
\]

$H$ is the \textbf{hat matrix} because it puts the hat on $y$. Adorable. It projects
$y$ onto $\langle X \rangle$ the column space of the design matrix $X$.
\[
    \hat{y} = X \hat{\beta} = X(X X^{-1})X'y = Hy
\]

\subsubsection{Error vector} is $y$ projected onto the space orthogonal to $X$.
\[
    e = y - \hat{y} = y - Hy = (I - H)y
\]

\subsection{Sums of Squares}

\subsubsection{SSE} - Error Sum of Squares. The degrees of freedom is the
length of the $\beta$ vector, which equals the number of regressors plus
one for the intercept.
\[
    \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y_i})^2
    = e'e
\]

\subsubsection{SSR} - Regression Sum of Squares
\[
    \sum_{i=1}^n (y_i - \bar{y})^2
    = (\hat{y} - \bar{y} 1)' (\hat{y} - \bar{y} 1) 
\]

SSTO - Total Sum of Squares, Define $J_n$ to be an $n$ by $n$ matrix of all 1's.
\[
    \sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n y_i^2 - n\bar{y}^2
    = y'(I - \frac{1}{n} J_n)y
\]

MSE - Error Mean Squared. SSE with degrees of freedom correction. In the
case of single regression with an intercept $df(SSE) = n-2$. This estimates
the variance of the errors.
\[
    MSE = SSE / df(SSE) \quad \E(MSE) = \sigma^2
\]

\subsection{Confidence Intervals}

\end{document}

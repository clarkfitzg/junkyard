% A simple template for LaTeX documents

\documentclass[12pt]{article}

% Begin paragraphs with new line
\usepackage{parskip}

% Change margin size
\usepackage[margin=1in]{geometry}

% Supports proof environment
\usepackage{amsthm}

% Allows writing \implies and align*
\usepackage{amsmath}

% Allows \mathbb{R}
\usepackage{amsfonts}

% Allows bold greek symbols \bm{\sigma} 
\usepackage{bm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% User defined commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\E}{\mathrm{E}}

\newcommand{\Var}{\mathrm{Var}}

\newcommand{\Cov}{\mathrm{Cov}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\section{Definitions}

\subsubsection{Model Assumptions}

The simple linear regression model with Normal errors assumes
\begin{itemize}
    \item $y = X \beta + \epsilon$ The response is a linear function of the predictors.
    \item $\E(\epsilon) = 0$ The expectation of the error term is 0.
    \item $\Cov(\epsilon) = \sigma^2 I_n$ The error terms have constant
variance $\sigma^2$ and are uncorrelated. This is equivalent to
$\Cov(y) = \sigma^2 I_n$.
\end{itemize}

The normal error model strengthens the second two conditions:
\begin{itemize}
    \item $\epsilon \sim $ Normal$(0, \sigma^2 I_n)$ The error terms are
normally distributed.
\end{itemize}

\subsubsection{Least Squares}

Least squares estimates $\beta$ by minimizing SSE, the error sum of
squares.
\[
    Q(\hat{\beta}) = e'e
\]

\subsubsection{Gauss Markov Theorem}

The least squares estimates are unbiased and have minimum variance of all
linear unbiased estimators.

\subsubsection{Design matrix}

$y$ can be decomposed into orthogonal vectors
\[
    y = \hat{y} + e.
\]

Estimated $\hat{\beta}$.
\[
    \hat{\beta} = (X'X)^{-1}X'y
\]

$H$ is the \textbf{hat matrix} because it puts the hat on $y$. Adorable. It projects
$y$ onto $\langle X \rangle$ the column space of the design matrix $X$.
\[
    \hat{y} = X \hat{\beta} = X(X X^{-1})X'y = Hy
\]

\subsubsection{Error vector} is $y$ projected onto the space orthogonal to $X$.
\[
    e = y - \hat{y} = y - Hy = (I - H)y
\]

\subsection{Sums of Squares}

\subsubsection{SXX} - Variation in X.
\[
    \sum_{i=1}^n (x_i - \bar{x})^2 
    = (x - \bar{x}1)' (x - \bar{x}1) 
    = x_c' x_c
\]

\subsubsection{SSE} - Error Sum of Squares. The degrees of freedom is the
length of the $\beta$ vector, which equals the number of regressors plus
one for the intercept.
\[
    \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y_i})^2
    = e'e
\]

\subsubsection{SSR} - Regression Sum of Squares
\[
    \sum_{i=1}^n (\hat{y_i} - \bar{y})^2
    = (\hat{y} - \bar{y} 1)' (\hat{y} - \bar{y} 1) 
    = \hat{y}_c' \hat{y}_c
\]

\subsubsection{SSTO} - Total Sum of Squares, Define $J_n$ to be an $n$ by $n$ matrix of all 1's.
\[
    \sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n y_i^2 - n\bar{y}^2
    = y'(I - \frac{1}{n} J_n)y
    = y_c' y_c
\]

\subsubsection{MSE} - Error Mean Squared. SSE with degrees of freedom correction. In the
case of single regression with an intercept $df(SSE) = n-2$. This estimates
the variance of the errors.
\[
    MSE = \frac{SSE}{df(SSE)} \quad \E(MSE) = \sigma^2
\]

\subsubsection{Decomposition of Error}
SSTO = SSE + SSR

\subsubsection{Measures of Fit}

$R^2$, the coefficient of multiple determination is the reduction of total
variation when using $X$ to explain $Y$.
\[
    R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Confidence Intervals}
The following results hold for the normal error model.

\subsubsection{Studentized pivotal quantity}
In the normal error model with one predictor, 
\[
    \frac{\hat{\beta_1} - \beta_1}{SE(\hat{\beta_1}}) \sim t_{n - 2}
\]
Which allows the construction of a $1 - \alpha$ confidence interval along
with the corresponding t tests.
\[
    \hat{\beta_1} + t(1 - \frac{\alpha}{2}; n - 2) \times
    SE(\hat{\beta_1})
\]

\subsubsection{Mean Response}
Estimates the average $y_h$ given $x_h$. 
\[
    SE^2(\hat{y_h}) = \sigma^2 (\frac{1}{n} + \frac{(x_h - \bar{x})^2}{x_c^t
x_c})
\]
A large range of $x$ or large $n$ makes for a tighter interval.

For multiple regression we have the variance of $\hat\beta$ is:
\[
    VAR(\hat\beta) = MSE (X'X)^{-1}.
\]
So to predict $\hat Y_h$ we have
\[
    s^2(\hat (Y_h) = MSE (X'_h (X'X)^{-1} X_H) .
\]
Then the $1 - \alpha$ confidence limits for $E(Y_h)$ are:
\[
    \hat Y_h +- t(1 - \alpha / 2; n - p) s(\hat Y _h)
\]

\subsubsection{Prediction}
Predicts $y_{h(new)}$ given $x_h$. 
\[
    SE^2(y_{h(new)}) = \sigma^2 + SE^2(\hat{y_h})
    = \sigma^2 ( 1 + \frac{1}{n} + \frac{(x_h - \bar{x})^2}{x_c^t x_c})
\]

\subsubsection{Estimating Whole line}
The \textbf{Working-Hotelling multiplier} gives a confidence band for the
entire regression line.
\[
    W = \sqrt{2F(1 - \alpha;2, n-2)}
\]
The $1 - \alpha$ confidence band is:
\[
    \hat{y_h} +- W \times SE(\hat{y_h}) \quad \forall x \in \mathbb{R}
\]

\subsection{Projection Matrices}
\textbf{Projection Matrices} are symmetric and idempotent. Here are the
ones we've seen:
\[
    H = X(X'X)^{-1} \quad \quad \quad I - H
\]
\end{document}

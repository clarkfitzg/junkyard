% A simple template for LaTeX documents
% 
% To produce pdf run:
%   $ pdflatex paper.tex 
%

\documentclass[12pt]{article}

% Begin paragraphs with new line
\usepackage{parskip}  

% Change margin size
\usepackage[margin=1in]{geometry}   

% Graphics Example:  (PDF's make for good plots)
\usepackage{graphicx}               
% \centerline{\includegraphics{figure.pdf}}

% Allows hyperlinks
\usepackage{hyperref}

% Blocks of code
\usepackage{listings}
\lstset{basicstyle=\ttfamily, title=\lstname}
% Insert code like this. replace `plot.R` with file name.
% \lstinputlisting{plot.R}

% Supports proof environment
\usepackage{amsthm}

% Allows writing \implies and align*
\usepackage{amsmath}

% Allows mathbb{R}
\usepackage{amsfonts}

% Allows alpha labels rather than numeric
\usepackage{enumitem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{center}
    {\Large Stats 206 - Homework 1}\\
    \bigskip
    \bigskip
    \hrule
    \medskip
    Clark Fitzgerald\\
    15 Oct 2014
\end{center}

\subsubsection*{2. True / False}
\begin{enumerate}[label=\alph*]
\item  \textbf{True} The least squares line always passes the center of the data.
This is implied by the formula
\[
    \hat{\beta_0} = \bar{Y} - \hat{\beta_1} \bar{X}.
\]
The large point in the below plot is the center of the data. The solid line
interpolating it is the fitted regression line.

\centerline{\includegraphics{regress.pdf}}

\item \textbf{False} The least squares line fits the data
best by definition. All other lines, including the true line, are worse
than the least square lines in this respect. 
In the figure above the fitted regression line is the solid line,
while the true regression line is the dashed line. They are not the same.

\item \textbf{True} $\bar{X} = 0$ and $\bar{Y} = 0 \implies \hat{\beta_0} =
0$. This is implied by the equation in part a).

\item \textbf{True} The standard errors for $\beta_0$ and $\beta_1$ both
have a term with $\sum (X_i - \bar{X})$ in the denominator. Hence increasing the
range of the $X_i$'s decreases the standard error estimates.

\item \textbf{True} The calculated standard error for predictions equals the 
standard error for mean plus an extra constant term,
implying that the prediction interval will be larger than that of the mean.

\item \textbf{False} A 95\% confidence interval represents our confidence
level that the true value lies in that interval. In these models we consider
$\beta_0$ to be a fixed parameter so we can't make this sort of probability
statement.

\item \textbf{True} One sided and two sided t tests are different, for
example.

\item \textbf{True} For most data sets there are generally fewer data points far from the mean,
which makes estimation more difficult. 

\item \textbf{True} Assuming that the regression coefficients are well 
defined, the least squares line will interpolate collinear points. This
implies that $y_i = \hat{y_i}$ for all $y_i \implies SSR = SSTO \implies R^2 = 1$.

\item \textbf{False} A small $R^2$ does not mean that the predictor and
response are not related; the relationship may be nonlinear. In the graph
below the points were generated from a sine function plus random normal
noise. The $R^2$ for the fitted line is nearly 0.

\centerline{\includegraphics{nonlinear.pdf}}

\item \textbf{False} It can be difficult to detect nonlinearity using a
scatterplot depending on the  scale of the variables.
Plotting the residuals of a linear model is more reliable.
An example was shown in the third lecture demonstrating this.

\end{enumerate}

\end{document}

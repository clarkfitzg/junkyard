---
title: Stats 206 - Homework 5
author: Clark Fitzgerald - 3013
date: 17 November 2014
output: pdf_document
fontsize: 10pt
geometry: margin=1in
---


```{r setup, include=FALSE}
# Use pdf plots rather than jpeg.
opts_chunk$set(dev = 'pdf')
library(MASS)
```

# 1

- (a) **FALSE** A categorical variable with $k$ classes should be encoded
   using $k - 1$ indicator variables. Using $k$ means that the columns of
   the matrix $X$ will be linearly dependent implying that the least
   squares solution is undefined.
   
- (b) **FALSE** Polynomials of high degree are notoriously unstable. This
  is why cubic splines are used for interpolation.

- (c) **TRUE** If $X_1$ and $X_2$ are two variables then their interaction
  term is $X_1 \cdot X_2$. By the hierarchical principle the model would
  then include $\beta_1 X_1 + \beta_2 X_2 + \beta_{1,2} X_1 X_2$.

- (d) **FALSE** Regressing on each class of a categorical model results in
  a smaller sample size $\implies$ higher error variance. It's also easier
  to understand the effect of that one categorical variable on the response
  using a single model.

- (e) **FALSE** This is not an efficient subset selection technique because
  of multicollinearity. This approach only works if the $X$ variables are
  mutually orthogonal.

- (f) **FALSE** 'Correct model' is a technical term meaning that the model
  has little bias. A model could be correct but not useful if the
  sampling variability is excessive due to many nuisance variables.

- (g) **FALSE** Including many nuisance $X$ variables results in higher
  variance and smaller bias.

- (h) **FALSE** Maximizing $R^2_p$ will result in always choosing the model
  with the most variables.

- (i) 

- (j) **TRUE** $C_p$ includes a positive $SSE_p$ coefficient and both
  $AIC_p$ and $BIC_p$ include a positive $log(SSE_p)$ term.

- (k) **TRUE** $SSE_p$ is the result of a global minimization with all of
  the data, meaning you can't possibly do better by leaving some data out.

- (l) **TRUE** If $\log(n) > 2 \iff n > 8$ then the coefficient for the $p$
  term is greater in $AIC_p$ than for $BIC_p$. Hence $BIC_p$ favors
  relatively smaller $p$.

- (m) **TRUE** Stepwise procedures are less computationally expensive
  compared with best subsets, so it makes sense to use them when there are
  many candidate $X$ variables.

- (n) **FALSE** Stepwise procedures are greedy, and they don't always
  find the globally optimal model.

# 2

## 2a

```{r, echo=FALSE, results='hide'}
setwd('~/junkyard/STA206/hw5')
property = read.table('../hw4/property.txt')
names(property) = c('Y', 'X1', 'X2', 'X3', 'X4')
```

We fit the new model with $X_1$ centered. For simplicity with the $\beta$
indices we let $\beta_5$ be the coefficient of the $X_{1c}^2$ term.
$$
    \hat{Y} = \beta_0 + \beta_1 X_{1c} + \beta_2 X_2 + \beta_4 X_4 + \beta_5 X_{1c}^2 
$$

```{r}
# Include the centered X1 variable
property$X1c = property$X1 - mean(property$X1)
fit1 = lm(Y ~ X1c + X2 + X4 + I(X1c**2), data=property)
summary(fit1)
```

```{r, Residuals, echo=FALSE}
plot(property$Y, fit1$residuals)
```

## 2b

Compare the measures of model fit.

```{r}
oldfit = lm(Y ~ X1 + X2 + X4, data = property)
measurefit = function(model){
    s = summary(model)
    data.frame(R2 = s$r.squared, R2a = s$adj.r.squared, BIC = BIC(model),
               AIC = AIC(model))
}
sapply(list('new'=fit1, 'old'=oldfit), measurefit)
```

The new model including the centered and quadratic term has
better measures of fit.

## 2c

We test whether the $X_{1c}^2$ term may be dropped from the model at level
0.05.
The null hypothesis $H_0$ is $\beta_5 = 0$, and the alternative hypothesis
$H_1$ is $\beta_5 \neq 0$. The test statistic is given by
$$
    T^* = \frac{\hat{\beta_5} - 0}{se(\hat{\beta_5})} \sim t(n - 5)
$$
under the null hypothesis.
The decision rule is to reject $H_0$ if $|T^*| > t(0.995, n - 5) \approx
2.64$. The p-value from the summary output above is 0.0174, so we reject $H_0$
and keep the quadratic term.

## 2d

Predicting a new observation

```{r}
x.new = data.frame(X1 = 4, X2 = 10, X4 = 8e4)
# Add the centered X1 term
x.new$X1c = x.new$X1 - mean(property$X1)

pnew = predict(fit1, x.new, interval = 'prediction', level = 0.99)
pnew
pold = predict(oldfit, x.new, interval = 'prediction', level = 0.99)
pold

pnew[3] - pnew[2]
pold[3] - pold[2]
```

The prediction intervals are similar. The last
calculation shows that the confidence intervals are slightly smaller for
the new model.

# 3

## 3a

Reading in the diabetes data.

```{r}
diabetes = read.table('diabetes.txt', header = TRUE)

# I don't like doing this because it leaves an empty string as a factor
# level. This may cause problems later.
is.na(diabetes$frame) = which(diabetes$frame == '')

fit = lm(glyhb ~ ratio + bp.1s + age + factor(gender) + factor(frame),
         na.action = na.omit, data = diabetes)
```

## 3b

```{r}
sapply(diabetes[, c('glyhb', 'ratio', 'bp.1s', 'age', 'gender', 'frame')],
       class)
```

Gender and frame are factors, the rest are numeric. Let's plot them.

```{r, histograms}
par(mfrow = c(2, 3))

plothelper = function(varname, plotfunc, data, ...){
    # Plots individual plots with the variable name
    # ... are additional arguments to `plotfunc`
    plotfunc(data[, varname], main = '',  xlab = varname, ...)
}

varnames = c('glyhb', 'ratio', 'bp.1s', 'age')
sapply(varnames, plothelper, hist, data=diabetes)

plot(diabetes$gender)
plot(diabetes$frame)
```

`Glyhb` is right skewed. `ratio` is clustered around 5, with a couple
outliers around 20. `bp.1s` looks close to normal. `age` is the most
uniformly distributed, with most between 20 and 80. There are slightly more
females than males. `frame` has small medium and large, with relatively
more mediums.

## 3c

Plot transformations for `glyhb`.
```{r, transformations}
par(mfrow = c(1, 3))
glyhb = data.frame(log = log(diabetes$glyhb), 
                   square.root = sqrt(diabetes$glyhb),
                   inverse = 1 / diabetes$glyhb)

sapply(c('log', 'square.root', 'inverse'), plothelper, hist, data=glyhb,
       bin)
```

The multiplicative inverse transformation looks like the closest to normal,
but that could be just because of the bin width.

## 3d

Scatterplots

```{r, scatterplots}
diabetes2 = diabetes[, c('glyhb', 'ratio', 'bp.1s', 'age')]
diabetes$glyhb.inverse = glyhb$inverse

plot(diabetes[, c('glyhb', 'ratio', 'bp.1s', 'age', 'glyhb.inverse')])
```

The transformation of `glyhb` seems to help with centering the data.

## 3e

Box plots for `glyhb`.

```{r, glyhb}
par(mfrow=c(2, 2))
boxplot(glyhb ~ gender, data = diabetes, main = 'glyhb and gender')
boxplot(glyhb ~ frame, data = diabetes, main = 'glyhb and frame')
boxplot(glyhb.inverse ~ gender, data = diabetes, main = 'transformed glyhb and gender')
boxplot(glyhb.inverse ~ frame, data = diabetes, main = 'transformed glyhb and frame')
```
The data appears to be more centered after the transformation.

## 3f

Fitting models

```{r, fit1}
par(mfrow=c(2, 1))
fit1 = lm(glyhb ~ ratio + bp.1s + age + gender + frame, data=diabetes)
plot.lm(fit1, which=c(1, 2))
```

The normal error assumption does not hold because the residuals are right
skewed.

## 3g

Box cox procedure on `glyhb`

```{r, boxcox, fig.height = 4}
boxcox(fit1)
```

The resulting plot shows that the multiplicative inverse was the best of
the available choices.

## 3h

Now we regress with the transformed `glyhb`

```{r, fit2}
par(mfrow=c(2, 1))
fit2 = lm(glyhb.inverse ~ ratio + bp.1s + age + gender + frame, data=diabetes)
plot.lm(fit2, which=c(1, 2))
```

These plots don't violate the model assumptions as before.
The errors still look a little heavy tailed though.

## 3i

```{r}
summary(fit2)
```

We drop `gender`, the least significant $X$ variable as judged by size of
the pvalue.

```{r}
fit3 = lm(glyhb.inverse ~ ratio + bp.1s + age + frame, data=diabetes)
summary(fit3)
```

## 3j

We drop `bp.1s`, the least significant $X$ variable as judged by size of
the pvalue.

```{r}
fit4 = lm(glyhb.inverse ~ ratio + age + frame, data=diabetes)
summary(fit4)
```

## 3k



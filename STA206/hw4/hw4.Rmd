---
title: Stats 206 - Homework 4
author: Clark Fitzgerald
date: 5 November 2014
output: pdf_document
fontsize: 11pt
geometry: margin=1in
---


```{r setup, include=FALSE}
# Use pdf plots rather than jpeg.
opts_chunk$set(dev = 'pdf')
library(plyr)
```

## 1a

We look at the correlation matrix:

```{r, scatterplot, echo=FALSE}
setwd('~/junkyard/STA206/hw4')
property = read.table('property.txt')
names(property) = c('Y', 'X1', 'X2', 'X3', 'X4')
plot(property)
cor(property)
```

There are no obviously strong linear relationships here. $X_2$ and $X_4$
exhibit the largest correlations.

## 1b

The least squares estimates, $R^2$, and $R^2_a$ can be read from the
model summary output:

```{r, echo=FALSE}
fit1 = lm(Y ~ ., data = property)
summary(fit1)
```

The fitted regression equation is:
$$
    Y = 12.2 - 0.142 X_1 + 0.282 X_2 + 0.619 X_3 + 7.92 \times 10^{-6} X_4
$$

MSE is 1.293 from the ANOVA table. 
```{r, echo=FALSE}
anova(fit1)
```

## 1c

```{r, Residuals, echo=FALSE, fig.height = 4}
par(mfrow=c(1, 3))

plot.lm(fit1, which=c(1, 2))
boxplot(fit1$residuals)
```

The residuals versus fitted graph shows no clear patterns, which is good.
We can see from the QQ plot that the tails of the distribution are light.

## 1d

```{r, Residuals and Main Effects, echo=FALSE}
par(mfrow=c(2, 2))

plotone = function(x) {
    plot(fit1$residuals, property[, x], xlab='residuals', ylab=x)
}

sapply(c('X1', 'X2', 'X3', 'X4'), plotone)
```

```{r, Residuals Versus Main Effects, echo=FALSE}
par(mfrow=c(3, 2))

plot.interact = function(xx) {
    # xx is a vector of the columns names to select
    d = property[, xx]
    interaction = d[, 1] * d[, 2]
    plot(fit1$residuals, interaction, xlab='residuals',
         ylab=paste(xx, collapse = ' * '))
}

apply(combn(c('X1', 'X2', 'X3', 'X4'), 2), 2, plot.interact)
```

TODO- Interpret these plots

## 1e

Testing whether each regression coefficient is 0  at level 0.01 with $p$ predictors
(including the intercept) is
similar to testing with a single predictor, we just use a $t$ distribution
with $n - p$ degrees of freedom. For $\hat{\beta_i}, i = 0, 1, 2, 3, 4$ the
null hypothesis $H_0$ is $\beta_i = 0$, and the alternative hypothesis
$H_1$ is $\beta_i \neq 0$. The test statistic is given by
$$
    T^*_i = \frac{\hat{\beta_i} - 0}{se(\hat{\beta_i})} \sim t(n - 5)
$$
under the null hypothesis. $se(\hat{\beta_i})$ is the $i + 1$ diagonal element of
$MSE(X'X)^{-1}$. 
The decision rule is to reject $H_0$ if $|T^*_i| > t(0.995, n - 5) \approx
2.64$. All of this information is available in the summary output:

```{r}
qt(0.995, 81 - 5)
summary(fit1)
```

We see that in the multiple regression model, only $X_3$ is
not significant. This implies that we could drop $X_3$ from the model.

## 1f

The ANOVA table below shows SSTO, SSR, and SSE.

```{r, echo=FALSE}
anova.table = function(model){

    # Makes an ANOVA table for a model resulting from lm output

    a = anova(model)

    total = data.frame('SS' = sum(a[, 'Sum Sq']), 'DF' = sum(a$Df))
    error = data.frame('SS' = a['Residuals', 'Sum Sq'], 
                       'DF' = a['Residuals', 'Df'])
    regression = data.frame(total - error)

    out = rbind(regression, error, total)
    out$MS = out$SS / out$DF
    row.names(out) = c('regression', 'error', 'total')
    return(out)
}
anova1 = anova.table(fit1)
anova1
```

To test whether there is a regression relation at $\alpha = 0.01$ we use an
F test. The null hypothesis $H_0$ is that $\beta_i = 0$ for $i = 1, 2, 3,
4$. $H_1$ is that not all such $\beta_i = 0$. The test statistic is 
$$
    F^* = \frac{MSR}{MSE} \sim_{H_0} F(4, 76)
$$

The decision rule is to reject $H_0$ if $F^* > F(0.99 ; 4, 80)$.

```{r}
qf99 = qf(0.99, 4, 76)
qf99

Fstar = anova1['regression', 'MS'] / anova1['error', 'MS']
Fstar

Fstar > qf99
```

Hence we reject $H_0$ and conclude that there is a significant regression relation at
$\alpha = 0.01$. Note that this information is available in the last line
of the `summary` output as well.

## 1g

Now we exclude $X_3$ from the model, because it failed the significance test in part e)
above.

```{r, echo=FALSE}
fit2 = lm(Y ~ X1 + X2 + X4, data = property)
summary(fit2)
anova(fit2)
```

We compare $MSE, R^2$ and $R^2_a$ with those of model 1.

```{r, echo=FALSE}
getr2 = function(modelname){
    model = get(modelname)
    r2 = summary(model)[c('r.squared', 'adj.r.squared')]

    data.frame(R2 = r2[1], R2a = r2[2], 
               MSE = anova(model)['Residuals', 'Mean Sq'],
               row.names = modelname)
}
sapply(c('fit1', 'fit2'), getr2)
```

The second fit has better adjusted $R^2_a$, and smaller MSE.

## 1h

We compare standard errors of the regression coefficient estimates 
for $X_1, X_2, X_4$ with those of model 1.

```{r, echo=FALSE}
getse = function(modelname){
    model = get(modelname)
    coef(summary(model))[c('X1', 'X2', 'X4'), 'Std. Error']
}
sapply(c('fit1', 'fit2'), getse)
```

As expected, Model 2 has smaller standard errors for each coefficient.
Here are 95% confidence intervals for the regression coefficients:

```{r}
confint(fit2, level=0.99)
```

The confidence intervals for Model 1 are larger, since Model 1 has larger
standard error around all regression coefficients.

```{r}
confint(fit1, level=0.99)
```

## 1i

We predict a new property under both models:

```{r}
x.new = data.frame(X1 = 4, X2 = 10, X3 = 0.1, X4 = 8e4)

predict(fit1, x.new, interval = 'prediction', level = 0.99)
predict(fit2, x.new, interval = 'prediction', level = 0.99)
```

The fitted values and intervals are similar with both models. The intervals
for Model 2 are marginally smaller.

## 1j

Model 2 is preferable. All of the exercises above demonstrate that
including the $X3$ variable does not help the fit in any significant way.
Therefore we choose the simpler model.

## 1k

We calculate the coefficient of partial determination 
$$
    R^2_{Y 3 | 124} = \frac{SSE(X_1, X_2, X_4) - SSE(X_1, X_2, X_3, X_4)}{SSE(X_1, X_2, X_4)}.
$$
This measures the relative change in SSE when the $X_3$ term is included.

```{r, echo=FALSE}
# Writing it in this order lets us see the effect of X3 after X4 is in the
# model. Use cumulative sums to calculate the difference.

a = anova(lm(Y ~ X1 + X2 + X4 + X3, data = property))
coefpd = a['X3', 'Sum Sq'] / a['Residuals', 'Sum Sq']
coefpd 
```

The coefficient of partial correlation $r_{Y 3 | 214}$ is:

```{r, echo=FALSE}
sqrt(coefpd) * sign(coef(fit1)['X4'])
```

The correlation coefficient between the two sets of residuals is:

```{r}
cor(fit1$residuals, fit2$residuals)
```

## 2a

```{r, boxplots, echo=FALSE}
par(mfrow = c(2, 5))
plothelper = function(varname, plotfunc, data=property, ...){
    # Plots individual plots with the variable name
    # ... are additional arguments to `plotfunc`
    plotfunc(data[, varname], main = varname, ...)
}
varnames = names(property)
sapply(varnames, plothelper, boxplot)
sapply(varnames, plothelper, hist, xlab='')
```

```{r}
summary(property)
```

The scale of the $X_4$ variable is much larger than all other variables.

## 2b

The sample means are available from the `summary` output above. The sample
standard deviations are:

```{r}
sapply(property, sd)
```

The sample means and sample standard deviations for the transformed
variables are:

```{r}
sapply(property.t, mean)
sapply(property.t, sd)
```

The means are numerically 0.

Here are some functions to perform the correlation transformation,
as well as the inverse transformation back to the original scale.

```{r}
getscale = function(X){
    # Returns scaling information
    # X is the original source of data
    # To be used together with `cor.transform`
    list(multiplier = 1 / sqrt(nrow(X) - 1),
         sd = sapply(X, sd),
         mean = sapply(X, mean))
}

cor.transform = function(X, scaleinfo, inverse=FALSE){
    # Perform a correlation transformation on X
    # X         : matrix
    # scaleinfo : output from getscale() on original data
    # inverse   : TRUE means to transform from standardized version
    #             standardized -> original

    # Create vectors from scaleinfo to work with R's recycling rules
    # Can't call nrow on vector X
    n = max(nrow(X), 1)
    s = lapply(scaleinfo, rep, each=n)

    if (inverse){
        Xnew = X * s$sd / s$multiplier + s$mean
    }
    else{
        Xnew = s$multiplier * (X - s$mean) / s$sd
    }
    return(Xnew)
}

scl = getscale(property)
property.t = cor.transform(property, scl)

# testing correctness
property.t2 = scale(property) / sqrt(nrow(property) - 1)
pback = cor.transform(property.t, scl, inverse=TRUE)

# testing works with vectors
a = as.vector(property[1, ])
at = cor.transform(a, scl)
cor.transform(at, scl, inverse=TRUE)
```

## 2c

The model equation for the standardized first-order regression model is
$$
    .   
$$

If we fit the model on the standardized data including the intercept we get:

```{r, echo=FALSE}
fit3 = lm(Y ~ ., data = property.t)
fit3
```

The intercept is numerically 0, as expected.

Now we fit the standardized data excluding the intercept.
```{r, echo=FALSE}
fit4 = lm(Y ~ . -1, data = property.t)
fit4
```

Transforming the standardized regression coefficients back to the ones for
the original model and compare with the original coefficients produces:

```{r}
std = coef(fit3) * sd(property$Y) / scl$sd
std[-1]
coef(fit1)
```

With the appropriate transformation we can recover the original
coefficients.

## 2d

Compare SSTO, SSE, and SSR under the standardized model with the original
model. The standardized model has one more degree of freedom when the
intercept is removed.

```{r, echo=FALSE}
# standardized
anova.table(fit4)
```

## 2e

$R^2$ and $R^2_a$ are available from the `summary` output.

```{r, echo=FALSE}
summary(fit4)
```

$R^2$ is the same for the standardized model and the original.

$R^2_a = 0.5632$ is slightly better for the standardized model versus
$R^2_a = 0.5629$ for the 

## 3a

We look at the correlation matrices. First we calculate through matrix
multiplication using the standardized variables.

```{r}
Xs = as.matrix(property.t)
rxx = t(Xs) %*% Xs
```

Here is the correlation between the original variables.
```{r}
cor(property)
```


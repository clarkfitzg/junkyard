## HW4

Stats 206

Clark Fitzgerald

```{r setup, include=FALSE}
# Use pdf plots rather than jpeg.
opts_chunk$set(dev = 'pdf')
library(plyr)
```

## 1a

```{r, scatterplot, echo=FALSE}
setwd('~/junkyard/STA206/hw4')
property = read.table('property.txt')
names(property) = c('Y', 'X1', 'X2', 'X3', 'X4')
plot(property)
cor(property)
```

There are no obviously strong linear relationships here. $X_2$ and $X_4$
exhibit the largest correlations.

## 1b

The least squares estimates, $R^2$, and $R^2_a$ can be read from the
model summary output:

```{r, echo=FALSE}
fit1 = lm(Y ~ ., data = property)
summary(fit1)
```

The fitted regression equation is:
$$
    Y = 12.2 - 0.142 X_1 + 0.282 X_2 + 0.619 X_3 + 7.92 \times 10^{-6} X_4
$$

MSE is 1.293 from the ANOVA table. 
```{r, echo=FALSE}
anova(fit1)
```

## 1c

```{r, Residuals, echo=FALSE, fig.height = 4}
par(mfrow=c(1, 3))

plot.lm(fit1, which=c(1, 2))
boxplot(fit1$residuals)
```

The residuals versus fitted graph shows no clear patterns, which is good.
We can see from the QQ plot that the tails of the distribution are light.

## 1d

```{r, Residuals and Main Effects, echo=FALSE}
par(mfrow=c(2, 2))

plotone = function(x) {
    plot(fit1$residuals, property[, x], xlab='residuals', ylab=x)
}

sapply(c('X1', 'X2', 'X3', 'X4'), plotone)
```

```{r, Residuals Versus Main Effects, echo=FALSE}
par(mfrow=c(3, 2))

plot.interact = function(xx) {
    # xx is a vector of the columns names to select
    d = property[, xx]
    interaction = d[, 1] * d[, 2]
    plot(fit1$residuals, interaction, xlab='residuals',
         ylab=paste(xx, collapse = ' * '))
}

apply(combn(c('X1', 'X2', 'X3', 'X4'), 2), 2, plot.interact)
```

TODO- Interpret these plots

## 1e

Testing whether each regression coefficient is 0  at level 0.01 with $p$ predictors
(including the intercept) is
similar to testing with a single predictor, we just use a $t$ distribution
with $n - p$ degrees of freedom. For $\hat{\beta_i}, i = 0, 1, 2, 3, 4$ the
null hypothesis $H_0$ is $\beta_i = 0$, and the alternative hypothesis
$H_1$ is $\beta_i \neq 0$. The test statistic is given by
$$
    T^*_i = \frac{\hat{\beta_i} - 0}{se(\hat{\beta_i})} \sim t(n - 5)
$$
under the null hypothesis. $se(\hat{\beta_i})$ is the $i + 1$ diagonal element of
$MSE(X'X)^{-1}$. 
The decision rule is to reject $H_0$ if $|T^*_i| > t(0.995, n - 5) \approx
2.64$. All of this information is available in the summary output:

```{r}
qt(0.995, 81 - 5)
summary(fit1)
```

We see that in the multiple regression model, only $X_4$ is
not significant. This implies that we could drop $X_4$ from the model.

## 4f

The ANOVA table below shows SSTO, SSR, and SSE.

```{r, echo=FALSE}
anova.table = function(model){

    # Makes an ANOVA table for a model resulting from lm output

    a = anova(model)

    total = data.frame('SS' = sum(a[, 'Sum Sq']), 'DF' = sum(a$Df))
    error = data.frame('SS' = a['Residuals', 'Sum Sq'], 
                       'DF' = a['Residuals', 'Df'])
    regression = data.frame(total - error)

    out = rbind(regression, error, total)
    out$MS = out$SS / out$DF
    row.names(out) = c('regression', 'error', 'total')
    return(out)
}
anova1 = anova(fit1)
anova.table(fit1)
```


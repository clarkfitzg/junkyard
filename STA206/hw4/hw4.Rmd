---
title: Stats 206 - Homework 4
author: Clark Fitzgerald - 3013
date: 5 November 2014
output: pdf_document
fontsize: 11pt
geometry: margin=1in
---


```{r setup, include=FALSE}
# Use pdf plots rather than jpeg.
opts_chunk$set(dev = 'pdf')
```

## 1a

We look at the correlation matrix:

```{r, echo=FALSE}
cor(property)
```

```{r, scatterplot, results='hide', echo=FALSE}
setwd('~/junkyard/STA206/hw4')
property = read.table('property.txt')
names(property) = c('Y', 'X1', 'X2', 'X3', 'X4')
plot(property)
```

There are no obviously strong linear relationships here. $X_2$ and $X_4$
exhibit the largest correlations.

## 1b

The least squares estimates, $R^2$, and $R^2_a$ can be read from the
model summary output:

```{r, echo=FALSE}
fit1 = lm(Y ~ ., data = property)
summary(fit1)
```

The fitted regression equation is:
$$
    Y = 12.2 - 0.142 X_1 + 0.282 X_2 + 0.619 X_3 + 7.92 \times 10^{-6} X_4
$$

MSE is 1.293 from the ANOVA table. 
```{r, echo=FALSE}
anova(fit1)
```

## 1c

```{r, Residuals, echo=FALSE, results='hide', fig.height = 4}
par(mfrow=c(1, 3))

plot.lm(fit1, which=c(1, 2))
boxplot(fit1$residuals)
```

The residuals versus fitted graph shows no clear patterns, which is good.
We can see from the QQ plot that the tails of the distribution are a little
bit heavy.

## 1d

```{r, Effects, results='hide', echo=FALSE}
par(mfrow=c(2, 2))

plotone = function(x) {
    plot(fit1$residuals, property[, x], xlab='residuals', ylab=x)
}

sapply(c('X1', 'X2', 'X3', 'X4'), plotone)
```

```{r, Interactions, results='hide', echo=FALSE}
par(mfrow=c(3, 2))

plot.interact = function(xx) {
    # xx is a vector of the columns names to select
    d = property[, xx]
    interaction = d[, 1] * d[, 2]
    plot(fit1$residuals, interaction, xlab='residuals',
         ylab=paste(xx, collapse = ' * '))
}

apply(combn(c('X1', 'X2', 'X3', 'X4'), 2), 2, plot.interact)
```

These plots show no obvious linear relationship between the residuals and other
terms, which suggests that we don't need the interaction terms

## 1e

Testing whether each regression coefficient is 0  at level 0.01 with $p$ predictors
(including the intercept) is
similar to testing with a single predictor, we just use a $t$ distribution
with $n - p$ degrees of freedom. For $\hat{\beta_i}, i = 0, 1, 2, 3, 4$ the
null hypothesis $H_0$ is $\beta_i = 0$, and the alternative hypothesis
$H_1$ is $\beta_i \neq 0$. The test statistic is given by
$$
    T^*_i = \frac{\hat{\beta_i} - 0}{se(\hat{\beta_i})} \sim t(n - 5)
$$
under the null hypothesis. $se(\hat{\beta_i})$ is the $i + 1$ diagonal element of
$MSE(X'X)^{-1}$. 
The decision rule is to reject $H_0$ if $|T^*_i| > t(0.995, n - 5) \approx
2.64$. All of this information is available in the summary output:

```{r}
qt(0.995, 81 - 5)
summary(fit1)
```

We see that in the multiple regression model, only $X_3$ is
not significant. This implies that we could drop $X_3$ from the model.

## 1f

The ANOVA table below shows SSTO, SSR, and SSE.

```{r, echo=FALSE}
anova.table = function(model){

    # Makes an ANOVA table for a model resulting from lm output

    a = anova(model)

    total = data.frame('SS' = sum(a[, 'Sum Sq']), 'DF' = sum(a$Df))
    error = data.frame('SS' = a['Residuals', 'Sum Sq'], 
                       'DF' = a['Residuals', 'Df'])
    regression = data.frame(total - error)

    out = rbind(regression, error, total)
    out$MS = out$SS / out$DF
    row.names(out) = c('regression', 'error', 'total')
    return(out)
}
anova1 = anova.table(fit1)
anova1
```

To test whether there is a regression relation at $\alpha = 0.01$ we use an
F test. The null hypothesis $H_0$ is that $\beta_i = 0$ for $i = 1, 2, 3,
4$. $H_1$ is that not all such $\beta_i = 0$. The test statistic is 
$$
    F^* = \frac{MSR}{MSE} \sim_{H_0} F(4, 76)
$$

The decision rule is to reject $H_0$ if $F^* > F(0.99 ; 4, 76)$.

```{r}
qf99 = qf(0.99, 4, 76)
qf99

Fstar = anova1['regression', 'MS'] / anova1['error', 'MS']
Fstar

Fstar > qf99
```

Hence we reject $H_0$ and conclude that there is a significant regression relation at
$\alpha = 0.01$. Note that this information is available in the last line
of the `summary` output as well.

## 1g

Now we exclude $X_3$ from the model, because it failed the significance test in part e)
above.

```{r, echo=FALSE}
fit2 = lm(Y ~ X1 + X2 + X4, data = property)
summary(fit2)
anova(fit2)
```

We compare $MSE, R^2$ and $R^2_a$ with those of model 1.

```{r, echo=FALSE}
getr2 = function(modelname){
    model = get(modelname)
    r2 = summary(model)[c('r.squared', 'adj.r.squared')]

    data.frame(R2 = r2[1], R2a = r2[2], 
               MSE = anova(model)['Residuals', 'Mean Sq'],
               row.names = modelname)
}
sapply(c('fit1', 'fit2'), getr2)
```

The second fit has better adjusted $R^2_a$, and smaller MSE.

## 1h

We compare standard errors of the regression coefficient estimates 
for $X_1, X_2, X_4$ with those of model 1.

```{r, echo=FALSE}
getse = function(modelname){
    model = get(modelname)
    coef(summary(model))[c('X1', 'X2', 'X4'), 'Std. Error']
}
sapply(c('fit1', 'fit2'), getse)
```

As expected, Model 2 has smaller standard errors for each coefficient.
Here are 95% confidence intervals for the regression coefficients:

```{r}
confint(fit2, level=0.95)
```

The confidence intervals for Model 1 are larger, since Model 1 has larger
standard error around all regression coefficients.

```{r}
confint(fit1, level=0.95)
```

## 1i

We predict a new property under both models:

```{r}
x.new = data.frame(X1 = 4, X2 = 10, X3 = 0.1, X4 = 8e4)

predict(fit1, x.new, interval = 'prediction', level = 0.99)
predict(fit2, x.new, interval = 'prediction', level = 0.99)
```

The fitted values and intervals are similar with both models. The intervals
for Model 2 are marginally smaller.

## 1j

Model 2 is preferable. All of the exercises above demonstrate that
including the $X3$ variable does not help the fit in any significant way.
Therefore we choose the simpler model.

## 1k

We calculate the coefficient of partial determination 
$$
    R^2_{Y 3 | 124} = \frac{SSE(X_1, X_2, X_4) - SSE(X_1, X_2, X_3, X_4)}{SSE(X_1, X_2, X_4)}.
$$
This measures the relative change in SSE when the $X_3$ term is included.

```{r, echo=FALSE}
# Writing it in this order lets us see the effect of X3 after X4 is in the
# model. Use cumulative sums to calculate the difference.

a = anova(lm(Y ~ X1 + X2 + X4 + X3, data = property))
coefpd = a['X3', 'Sum Sq'] / a['Residuals', 'Sum Sq']
coefpd 
```

The coefficient of partial correlation $r_{Y 3 | 214}$ is:

```{r, echo=FALSE}
cpc = sqrt(coefpd) * sign(coef(fit1)['X4'])
names(cpc) = ''
cpc
```

The correlation coefficient between the two sets of residuals is:

```{r}
cor(fit1$residuals, fit2$residuals)
```

## 2a

```{r, boxplots, results='hide', echo=FALSE}
par(mfrow = c(2, 5))
plothelper = function(varname, plotfunc, data=property, ...){
    # Plots individual plots with the variable name
    # ... are additional arguments to `plotfunc`
    plotfunc(data[, varname], main = varname, ...)
}
varnames = names(property)
sapply(varnames, plothelper, boxplot)
sapply(varnames, plothelper, hist, xlab='')
```

```{r}
summary(property)
```

The scale of the $X_4$ variable is much larger than all other variables.

## 2b

The sample means are available from the `summary` output above. The sample
standard deviations are:

```{r}
sapply(property, sd)
```

The sample means and sample standard deviations for the transformed
variables are:

```{r}
sapply(property.t, mean)
sapply(property.t, sd)
```

The means are numerically 0.

Here are some functions to perform the correlation transformation,
as well as the inverse transformation back to the original scale.

```{r}
getscale = function(X){
    # Returns scaling information
    # X is the original source of data
    # To be used together with `cor.transform`
    list(multiplier = 1 / sqrt(nrow(X) - 1),
         sd = sapply(X, sd),
         mean = sapply(X, mean))
}

cor.transform = function(X, scaleinfo, inverse=FALSE){
    # Perform a correlation transformation on X
    # X         : matrix
    # scaleinfo : output from getscale() on original data
    # inverse   : TRUE means to transform from standardized version
    #             standardized -> original

    # Create vectors from scaleinfo to work with R's recycling rules
    # Can't call nrow on vector X
    n = max(nrow(X), 1)
    s = lapply(scaleinfo, rep, each=n)

    if (inverse){
        Xnew = X * s$sd / s$multiplier + s$mean
    }
    else{
        Xnew = s$multiplier * (X - s$mean) / s$sd
    }
    return(Xnew)
}

scl = getscale(property)
property.t = cor.transform(property, scl)

# testing correctness
property.t2 = scale(property) / sqrt(nrow(property) - 1)
pback = cor.transform(property.t, scl, inverse=TRUE)

# testing works with vectors
a = as.vector(property[1, ])
at = cor.transform(a, scl)
cor.transform(at, scl, inverse=TRUE)
```

## 2c

The model equation for the standardized first-order regression model is
$$
    .   
$$

If we fit the model on the standardized data including the intercept we get:

```{r, echo=FALSE}
fit3 = lm(Y ~ ., data = property.t)
fit3
```

The intercept is numerically 0, as expected.

Now we fit the standardized data excluding the intercept.
```{r, echo=FALSE}
fit4 = lm(Y ~ . -1, data = property.t)
fit4
```

Transforming the standardized regression coefficients back to the ones for
the original model and compare with the original coefficients produces:

```{r}
std = coef(fit3) * sd(property$Y) / scl$sd
std[-1]
coef(fit1)
```

With the appropriate transformation we can recover the original
coefficients.

## 2d

Compare SSTO, SSE, and SSR under the standardized model with the original
model. The standardized model has one more degree of freedom when the
intercept is removed.

```{r, echo=FALSE}
# standardized
anova.table(fit4)
```

## 2e

$R^2$ and $R^2_a$ are available from the `summary` output.

```{r, echo=FALSE}
summary(fit4)
```

$R^2$ is the same for the standardized model and the original. $R^2_a$ is
marginally better for the standardized model excluding the intercept,
because R considers it as using one less degree of freedom. This is not
true however, because the correlation transformation centered the data,
projecting it into the subspace orthogonal to the vector of 1's.

## 3a

We look at the correlation matrices. First we calculate through matrix
multiplication using the standardized variables.

```{r}
Xs = as.matrix(property.t[, 2:5])
Ys = as.matrix(property.t[, 1])
rxx = t(Xs) %*% Xs
rxy = t(Xs) %*% Ys
rxx
rxy
```

Here is the correlation between the original variables.
```{r}
cor(property)
```
They match the matrix calculations.

## 3b

The variance inflator factors are:

```{r}
rxxinv = solve(rxx)
diag(rxxinv)
```

We confirm that $VIF_k = \frac{1}{1 - R_k^2}$ by regressing each $X_k$ on
the other $X_j \neq X_k$.

```{r, echo=FALSE}
mods = list()
mods$X1 = lm(X1 ~ X2 + X3 + X4, data=property)
mods$X2 = lm(X2 ~ X1 + X3 + X4, data=property)
mods$X3 = lm(X3 ~ X1 + X2 + X4, data=property)
mods$X4 = lm(X4 ~ X1 + X2 + X3, data=property)
sapply(mods, function(x) 1 / (1 - summary(x)$r.squared))
```

The rule of thumb is that if max $VIF_k > 10$ then multicollinearity is a
cause for concern. We don't observe that here.

## 3c

```{r, echo=FALSE}
mod.X4 = lm(Y ~ X4, data=property)
mod.X3X4 = lm(Y ~ X3 + X4, data=property)
mod.X4
mod.X3X4
```

The regression coefficients for $X_4$ are similar if $X_3$ is included or
excluded. This does not surprise us, since $X_3$ and $X_4$ are not highly
correlated.

```{r, echo=FALSE}
anova(mod.X4)
anova(mod.X3X4)
```
From the ANOVA output we can see that $X_3$ makes almost no difference in
SSR.

## 3d

```{r, echo=FALSE}
mod.X2 = lm(Y ~ X2, data=property)
mod.X2X4 = lm(Y ~ X4 + X2, data=property)
mod.X2
mod.X2X4
```

$X_2$ and $X_4$ have sample correlation 0.44, and we can see that including
$X_4$ in the model makes a large change in the regression coefficient for
$X_2$. 

```{r, echo=FALSE}
anova(mod.X2)
anova(mod.X2X4)
```

From the ANOVA table we see that $SSR(X_2 | X_4)$ is small. This is due to
the collinearity of $X_2$ and $X_4$.

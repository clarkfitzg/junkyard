---
title: Stats 206 - Homework 4
author: Clark Fitzgerald
date: 5 November 2014
output: pdf_document
fontsize: 11pt
geometry: margin=1in
---


```{r setup, include=FALSE}
# Use pdf plots rather than jpeg.
opts_chunk$set(dev = 'pdf')
library(plyr)
```

## 1a

We look at the correlation matrix:

```{r, scatterplot, echo=FALSE}
setwd('~/junkyard/STA206/hw4')
property = read.table('property.txt')
names(property) = c('Y', 'X1', 'X2', 'X3', 'X4')
plot(property)
cor(property)
```

There are no obviously strong linear relationships here. $X_2$ and $X_4$
exhibit the largest correlations.

## 1b

The least squares estimates, $R^2$, and $R^2_a$ can be read from the
model summary output:

```{r, echo=FALSE}
fit1 = lm(Y ~ ., data = property)
summary(fit1)
```

The fitted regression equation is:
$$
    Y = 12.2 - 0.142 X_1 + 0.282 X_2 + 0.619 X_3 + 7.92 \times 10^{-6} X_4
$$

MSE is 1.293 from the ANOVA table. 
```{r, echo=FALSE}
anova(fit1)
```

## 1c

```{r, Residuals, echo=FALSE, fig.height = 4}
par(mfrow=c(1, 3))

plot.lm(fit1, which=c(1, 2))
boxplot(fit1$residuals)
```

The residuals versus fitted graph shows no clear patterns, which is good.
We can see from the QQ plot that the tails of the distribution are light.

## 1d

```{r, Residuals and Main Effects, echo=FALSE}
par(mfrow=c(2, 2))

plotone = function(x) {
    plot(fit1$residuals, property[, x], xlab='residuals', ylab=x)
}

sapply(c('X1', 'X2', 'X3', 'X4'), plotone)
```

```{r, Residuals Versus Main Effects, echo=FALSE}
par(mfrow=c(3, 2))

plot.interact = function(xx) {
    # xx is a vector of the columns names to select
    d = property[, xx]
    interaction = d[, 1] * d[, 2]
    plot(fit1$residuals, interaction, xlab='residuals',
         ylab=paste(xx, collapse = ' * '))
}

apply(combn(c('X1', 'X2', 'X3', 'X4'), 2), 2, plot.interact)
```

TODO- Interpret these plots

## 1e

Testing whether each regression coefficient is 0  at level 0.01 with $p$ predictors
(including the intercept) is
similar to testing with a single predictor, we just use a $t$ distribution
with $n - p$ degrees of freedom. For $\hat{\beta_i}, i = 0, 1, 2, 3, 4$ the
null hypothesis $H_0$ is $\beta_i = 0$, and the alternative hypothesis
$H_1$ is $\beta_i \neq 0$. The test statistic is given by
$$
    T^*_i = \frac{\hat{\beta_i} - 0}{se(\hat{\beta_i})} \sim t(n - 5)
$$
under the null hypothesis. $se(\hat{\beta_i})$ is the $i + 1$ diagonal element of
$MSE(X'X)^{-1}$. 
The decision rule is to reject $H_0$ if $|T^*_i| > t(0.995, n - 5) \approx
2.64$. All of this information is available in the summary output:

```{r}
qt(0.995, 81 - 5)
summary(fit1)
```

We see that in the multiple regression model, only $X_3$ is
not significant. This implies that we could drop $X_3$ from the model.

## 1f

The ANOVA table below shows SSTO, SSR, and SSE.

```{r, echo=FALSE}
anova.table = function(model){

    # Makes an ANOVA table for a model resulting from lm output

    a = anova(model)

    total = data.frame('SS' = sum(a[, 'Sum Sq']), 'DF' = sum(a$Df))
    error = data.frame('SS' = a['Residuals', 'Sum Sq'], 
                       'DF' = a['Residuals', 'Df'])
    regression = data.frame(total - error)

    out = rbind(regression, error, total)
    out$MS = out$SS / out$DF
    row.names(out) = c('regression', 'error', 'total')
    return(out)
}
anova1 = anova.table(fit1)
anova1
```

To test whether there is a regression relation at $\alpha = 0.01$ we use an
F test. The null hypothesis $H_0$ is that $\beta_i = 0$ for $i = 1, 2, 3,
4$. $H_1$ is that not all such $\beta_i = 0$. The test statistic is 
$$
    F^* = \frac{MSR}{MSE} \sim_{H_0} F(4, 76)
$$

The decision rule is to reject $H_0$ if $F^* > F(0.99 ; 4, 80)$.

```{r}
qf99 = qf(0.99, 4, 76)
qf99

Fstar = anova1['regression', 'MS'] / anova1['error', 'MS']
Fstar

Fstar > qf99
```

Hence we reject $H_0$ and conclude that there is a significant regression relation at
$\alpha = 0.01$. Note that this information is available in the last line
of the `summary` output as well.

## 1g

Now we exclude $X_3$ from the model, because it failed the significance test in part e)
above.

```{r, echo=FALSE}
fit2 = lm(Y ~ X1 + X2 + X4, data = property)
summary(fit2)
anova(fit2)
```

We compare $MSE, R^2$ and $R^2_a$ with those of model 1.

```{r, echo=FALSE}
getr2 = function(modelname){
    model = get(modelname)
    r2 = summary(model)[c('r.squared', 'adj.r.squared')]

    data.frame(R2 = r2[1], R2a = r2[2], 
               MSE = anova(model)['Residuals', 'Mean Sq'],
               row.names = modelname)
}
sapply(c('fit1', 'fit2'), getr2)
```

The second fit has better adjusted $R^2_a$, and smaller MSE.
